{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa8fb71-87ce-4215-a623-677adad430cd",
   "metadata": {},
   "source": [
    "## Project GERALD\n",
    "\n",
    "### Objective\n",
    "\n",
    "\n",
    "\n",
    "#### Authors: Suman Senapati\n",
    "####          Sanjay Rao\n",
    "####          Pratibha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8935028-75cf-407f-a100-de075c9f06dc",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd05bb5a-30e0-461a-a8c7-5e310f1728c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict, Counter\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Set, Tuple, Optional, Any\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import Dict, List, Set, Tuple, Optional, Any\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, MobileNetV2, EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.utils as keras_utils\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d9079-51fa-4b15-b174-eb24803f695b",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90153da-2143-437d-b5ba-423b74cfec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"../dataset/GERALD_subset\"\n",
    "TARGET_MB = 2400\n",
    "CLASSES_KEEP = [\"main_signal\", \"distant_signal\"]\n",
    "\n",
    "CATEGORY_ALIASES = {\n",
    "    \"main_signal\": [\"Hp_0_HV\", \"Hp_1\", \"Hp_2\", \"Hp_0_Sh\", \"Ks_1\", \"Ks_2\"],\n",
    "    \"distant_signal\": [\"Vr_0\", \"Vr_1\", \"Vr_2\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caddfad9-69fb-4f3b-a24e-ca1e2f257a80",
   "metadata": {},
   "source": [
    "## Helper classes\n",
    "\n",
    "#####  Class to create a balanced subset of the GERALD dataset under different weather and light conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1869d2-0c9b-4c10-9ada-221b037b4d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeraldSignalSubsetCreator:\n",
    "\n",
    "    \"\"\"\n",
    "    Class to create a balanced subset of the GERALD dataset under different weather and light conditions.\n",
    "    \n",
    "    Attributes:\n",
    "        out_dir (Path): Output directory for the subset.\n",
    "        target_bytes (int): Maximum target dataset size in bytes.\n",
    "        current_bytes (int): Current accumulated dataset size in bytes.\n",
    "        class_stats (defaultdict): Counter for objects per class.\n",
    "        all_classes (set): Set of all classes detected in the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.out_dir = Path(OUT_DIR)\n",
    "        self.target_bytes = TARGET_MB * 1024 * 1024\n",
    "        self.current_bytes = 0\n",
    "        self.class_stats = defaultdict(int)\n",
    "        self.all_classes = set()\n",
    "\n",
    "        (self.out_dir / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "        (self.out_dir / \"annotations\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Subset Creation\n",
    "    def create_subset(self, zip_path: Path):\n",
    "\n",
    "        \"\"\"\n",
    "        Creates a balanced subset from the GERALD dataset ZIP file.\n",
    "        \n",
    "        Args:\n",
    "            zip_path (Path): Path to the GERALD.zip archive.\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: If info.json is missing or class mapping fails.\n",
    "            AssertionError: If dataset validation fails.\n",
    "        \"\"\"\n",
    "        mapping = {}\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "\n",
    "            # Locate info.json inside GERALD/ directory\n",
    "            info_candidates = [\n",
    "                f for f in zf.namelist()\n",
    "                if f.lower().endswith(\"info.json\")\n",
    "            ]\n",
    "\n",
    "            if not info_candidates:\n",
    "                raise RuntimeError(\"info.json not found anywhere inside GERALD.zip!\")\n",
    "\n",
    "            info_path = info_candidates[0]\n",
    "            print(\"Found info.json at:\", info_path)\n",
    "\n",
    "            raw = zf.read(info_path)\n",
    "\n",
    "            # Encoding fallback\n",
    "            for enc in [\"utf-8\", \"utf-16\", \"latin1\", \"cp1252\"]:\n",
    "                try:\n",
    "                    info_json = json.loads(raw.decode(enc))\n",
    "                    print(\"Loaded info.json successfully with:\", enc)\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            else:\n",
    "                raise RuntimeError(\"Could not decode info.json in any common encoding\")\n",
    "\n",
    "            (self.out_dir / \"info.json\").write_bytes(raw)\n",
    "\n",
    "            # Collect Annotation XML files\n",
    "            xml_files = [f for f in zf.namelist() if f.lower().endswith(\".xml\")]\n",
    "\n",
    "            print(\"Scanning XMLs to collect class names!!!\")\n",
    "            for xml_file in tqdm(xml_files, desc=\"Scanning XMLs\"):\n",
    "                try:\n",
    "                    with zf.open(xml_file) as f:\n",
    "                        root = ET.parse(f).getroot()\n",
    "                        for obj in root.findall(\"object\"):\n",
    "                            self.all_classes.add(obj.find(\"name\").text.strip())\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            # Build alias mapping\n",
    "            for target_class, aliases in CATEGORY_ALIASES.items():\n",
    "                for cls in self.all_classes:\n",
    "                    if any(a.lower() in cls.lower() for a in aliases):\n",
    "                        mapping[cls] = target_class\n",
    "\n",
    "            if not mapping:\n",
    "                raise RuntimeError(\"No classes matched alias rules!\")\n",
    "\n",
    "            # Build weather/light mapping\n",
    "            print(\"Indexing weather/light combinations!!!\")\n",
    "            wls = defaultdict(list)\n",
    "\n",
    "            for xml_file in tqdm(xml_files, desc=\"Identifying weather/light\"):\n",
    "                try:\n",
    "                    with zf.open(xml_file) as f:\n",
    "                        root = ET.parse(f).getroot()\n",
    "\n",
    "                    img_name = root.find(\"filename\").text.strip()\n",
    "\n",
    "                    if img_name not in info_json:\n",
    "                        continue\n",
    "\n",
    "                    weather = info_json[img_name][\"weather\"]\n",
    "                    light = info_json[img_name][\"light\"]\n",
    "\n",
    "                    wls[(weather, light)].append(xml_file)\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            # Plot the distribution\n",
    "            self.plot_distribution(wls)\n",
    "\n",
    "\n",
    "            # Auto tuning logic ->Auto tune max per-pair \n",
    "            #---- logic ------------------------------\n",
    "            # 1. Compute the size of each weather/light group.\n",
    "            # 2. Sort the group sizes to find the median group size.\n",
    "            # 3. If the number of groups is odd, take the middle value as the median.\n",
    "            #    If even, take the average of the two middle values.\n",
    "            # 4. Set MAX_PER_PAIR as the median group size but restrict it to a minimum of 50\n",
    "            #    and a maximum of 300 to avoid selecting too few or too many samples.\n",
    "            # This ensures a balanced and reasonable selection of images per weather/light combination.\n",
    "\n",
    "            # ----------------------------------------\n",
    "\n",
    "            print(\"\\nAuto-tuning max samples per weather/light pair!!!!!\")\n",
    "\n",
    "            group_sizes = [len(v) for v in wls.values()]\n",
    "            group_sizes_sorted = sorted(group_sizes)\n",
    "\n",
    "            mid = len(group_sizes_sorted) // 2\n",
    "            if len(group_sizes_sorted) % 2 == 1:\n",
    "                median_size = group_sizes_sorted[mid]\n",
    "            else:\n",
    "                median_size = (group_sizes_sorted[mid - 1] + group_sizes_sorted[mid]) // 2\n",
    "\n",
    "            MAX_PER_PAIR = max(50, min(median_size, 300))\n",
    "\n",
    "            print(f\"Group sizes: {group_sizes}\")\n",
    "            print(f\"Median = {median_size}\")\n",
    "            print(f\"Auto-selected MAX_PER_PAIR = {MAX_PER_PAIR}\")\n",
    "\n",
    "\n",
    "            # Randomly select XML files for each weather/light group.\n",
    "            # For each group:\n",
    "                # 1. Shuffle the XML list to ensure random selection.\n",
    "                # 2. Take up to MAX_PER_PAIR files (or all if the group is smaller).\n",
    "                # 3. Add them to the final selection.\n",
    "            \n",
    "            selected_xmls = []\n",
    "\n",
    "            for pair, xml_list in wls.items():\n",
    "                xml_list = xml_list.copy()\n",
    "                random.shuffle(xml_list)           # RANDOM PICK INSIDE EACH GROUP\n",
    "                take_n = min(len(xml_list), MAX_PER_PAIR)\n",
    "                selected_xmls.extend(xml_list[:take_n])\n",
    "\n",
    "            print(f\"Total images selected = {len(selected_xmls)}\")\n",
    "\n",
    "            # Copy balanced dataset and embed metadata\n",
    "            print(\"\\nCopying balanced dataset!!!!\")\n",
    "\n",
    "            with tqdm(total=self.target_bytes, unit=\"B\", unit_scale=True, desc=\"Copying dataset\") as pbar:\n",
    "                for xml_file in selected_xmls:\n",
    "                    if self.current_bytes >= self.target_bytes:\n",
    "                        break\n",
    "\n",
    "                    try:\n",
    "                        with zf.open(xml_file) as f:\n",
    "                            tree = ET.parse(f)\n",
    "                            root = tree.getroot()\n",
    "\n",
    "                        filtered = []\n",
    "                        for obj in root.findall(\"object\"):\n",
    "                            name = obj.find(\"name\").text.strip()\n",
    "                            if name in mapping:\n",
    "                                obj.find(\"name\").text = mapping[name]\n",
    "                                filtered.append(obj)\n",
    "\n",
    "                        if not filtered:\n",
    "                            continue\n",
    "\n",
    "                        # rewrite XML with filtered objects only\n",
    "                        for obj in root.findall(\"object\"):\n",
    "                            root.remove(obj)\n",
    "                        for obj in filtered:\n",
    "                            root.append(obj)\n",
    "\n",
    "                        img_name = root.find(\"filename\").text.strip()\n",
    "\n",
    "                        # metadata injection happens here - > (weather + light only)\n",
    "                        meta = ET.SubElement(root, \"metadata\")\n",
    "                        ET.SubElement(meta, \"weather\").text = info_json[img_name][\"weather\"]\n",
    "                        ET.SubElement(meta, \"light\").text = info_json[img_name][\"light\"]\n",
    "\n",
    "                        # Find and copy image\n",
    "                        img_file_zip = next(\n",
    "                            (f for f in zf.namelist() if f.endswith(img_name)),\n",
    "                            None\n",
    "                        )\n",
    "\n",
    "                        if not img_file_zip:\n",
    "                            continue\n",
    "\n",
    "                        img_bytes = zf.read(img_file_zip)\n",
    "\n",
    "                        if self.current_bytes + len(img_bytes) > self.target_bytes:\n",
    "                            break\n",
    "\n",
    "                        (self.out_dir / \"images\" / img_name).write_bytes(img_bytes)\n",
    "                        tree.write(self.out_dir / \"annotations\" / Path(xml_file).name)\n",
    "\n",
    "                        for obj in filtered:\n",
    "                            self.class_stats[obj.find(\"name\").text.strip()] += 1\n",
    "\n",
    "                        self.current_bytes += len(img_bytes)\n",
    "                        pbar.update(len(img_bytes))\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping {xml_file} due to {e}\")\n",
    "\n",
    "        # Save mapping and stats\n",
    "        reverse_map = defaultdict(list)\n",
    "        for original, target in mapping.items():\n",
    "            reverse_map[target].append(original)\n",
    "\n",
    "        with open(self.out_dir / \"class_mapping.json\", \"w\") as f:\n",
    "            json.dump({\n",
    "                \"target_classes\": CLASSES_KEEP,\n",
    "                \"gerald_to_target_mapping\": mapping,\n",
    "                \"target_to_gerald_mapping\": dict(reverse_map),\n",
    "                \"class_statistics\": dict(self.class_stats),\n",
    "            }, f, indent=2)\n",
    "\n",
    "        print(\"\\nSubset creation completed.\")\n",
    "        print(f\"Final dataset size: {self.current_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        self.validate_subset()\n",
    "\n",
    "\n",
    "    def plot_distribution(self, wls):\n",
    "        \"\"\"\n",
    "        Plots a bar chart showing the number of samples per weather/light combination\n",
    "        and saves the figure as 'subset_distribution.png' in the output directory.\n",
    "\n",
    "        Args:\n",
    "            wls (dict): Dictionary mapping (weather, light) pairs to lists of XML files.\n",
    "        \"\"\"\n",
    "        \n",
    "        pairs = [f\"{w}-{l}\" for (w, l) in wls.keys()]\n",
    "        counts = [len(wls[p]) for p in wls.keys()]\n",
    "\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.bar(pairs, counts, color='steelblue')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.title(\"Weather Ã— Light Distribution\")\n",
    "        plt.ylabel(\"Sample Count\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(self.out_dir / \"subset_distribution.png\")\n",
    "        plt.show()\n",
    "\n",
    "    # Subset Validation\n",
    "    \n",
    "    def validate_subset(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Validates the generated subset for consistency:\n",
    "        - All images exist\n",
    "        - All object classes are correct\n",
    "        - Class statistics match JSON metadata\n",
    "        Raises:\n",
    "            AssertionError: If any validation check fails.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\n=== VALIDATING DATASET ===\")\n",
    "\n",
    "        ann_dir = self.out_dir / \"annotations\"\n",
    "        img_dir = self.out_dir / \"images\"\n",
    "        mapping_path = self.out_dir / \"class_mapping.json\"\n",
    "\n",
    "        with open(mapping_path) as f:\n",
    "            m = json.load(f)\n",
    "\n",
    "        target_classes = set(m[\"target_classes\"])\n",
    "        class_stats_json = m[\"class_statistics\"]\n",
    "\n",
    "        \"\"\"\n",
    "        i. Collects all XML annotation files in the annotations directory.\n",
    "        ii. Initializes counters to track object counts per class, a list for missing images, \n",
    "           and a flag to track overall validation success.\n",
    "        iii. Iterates through each XML file to:\n",
    "           - Ensure the corresponding image exists; if not, mark it as missing.\n",
    "           - Count all objects per class and check that each object belongs to a valid target class.\n",
    "        iv. Compares the counted objects with the stored class statistics from JSON to ensure \n",
    "           consistency between the XML annotations and metadata.\n",
    "        v. Flags any missing images, invalid classes, or count mismatches as validation failures.\n",
    "        \"\"\"\n",
    "\n",
    "        xml_files = list(ann_dir.glob(\"*.xml\"))\n",
    "        counter = Counter()\n",
    "        missing_images = []\n",
    "        all_good = True\n",
    "\n",
    "        for xml_file in xml_files:\n",
    "            root = ET.parse(xml_file).getroot()\n",
    "            img_name = root.find(\"filename\").text\n",
    "\n",
    "            if not (img_dir / img_name).exists():\n",
    "                missing_images.append(img_name)\n",
    "                all_good = False\n",
    "\n",
    "            for obj in root.findall(\"object\"):\n",
    "                cls = obj.find(\"name\").text.strip()\n",
    "                counter[cls] += 1\n",
    "\n",
    "                if cls not in target_classes:\n",
    "                    print(f\"Invalid class '{cls}' in {xml_file.name}\")\n",
    "                    all_good = False\n",
    "\n",
    "        if counter != Counter(class_stats_json):\n",
    "            print(\"Class statistics mismatch!\")\n",
    "            print(\"Counts from XML:\", dict(counter))\n",
    "            print(\"Counts in JSON:\", class_stats_json)\n",
    "            all_good = False\n",
    "\n",
    "        print(\"\\n=== DATASET SUMMARY ===\")\n",
    "        print(f\"Images saved: {len(list(img_dir.glob('*')))}\")\n",
    "        print(f\"Annotations: {len(xml_files)}\")\n",
    "        print(f\"Total objects: {sum(counter.values())}\")\n",
    "\n",
    "        print(\"\\nObjects per class:\")\n",
    "        for cls, count in counter.items():\n",
    "            print(f\"  - {cls}: {count}\")\n",
    "\n",
    "        print(f\"\\nSubset size: {self.current_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        if missing_images:\n",
    "            print(\"Missing images:\", missing_images)\n",
    "            all_good = False\n",
    "\n",
    "        if all_good:\n",
    "            print(\"\\n All checks passed! Dataset is fully consistent!\\n\")\n",
    "        else:\n",
    "            raise AssertionError(\"Dataset validation failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72234c9-a336-4640-a57d-dd39ed868b60",
   "metadata": {},
   "source": [
    "## Subset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d9704b-f4a8-421c-8932-d0a2b2645378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GERALD.zip not found!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    zip_path = Path(\"../dataset/GERALD.zip\")\n",
    "    if not zip_path.exists():\n",
    "        print(\"GERALD.zip not found!\")\n",
    "    else:\n",
    "        GeraldSignalSubsetCreator().create_subset(zip_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba4859-bb43-412d-b046-8c0f80ea8dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"../dataset/GERALD_subset\")\n",
    "IMG_DIR = BASE_DIR / \"images\"\n",
    "ANN_DIR = BASE_DIR / \"annotations\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666465e9-ef7d-4cb6-bccb-de7fd079a139",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ab827-98be-4ead-a9a2-fa6be435b950",
   "metadata": {},
   "source": [
    "#### EDA Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e6f4e-0b53-49c7-a68c-b277fded8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(ann_dir):\n",
    "    \"\"\"\n",
    "    Parses all XML files in the annotation directory and creates a DataFrame.\n",
    "    \"\"\"\n",
    "    xml_list = []\n",
    "    xml_files = list(ann_dir.glob(\"*.xml\"))\n",
    "    \n",
    "    print(f\"Parsing {len(xml_files)} annotation files...\")\n",
    "\n",
    "    for xml_file in xml_files:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        filename = root.find('filename').text\n",
    "        img_width = int(root.find('size').find('width').text)\n",
    "        img_height = int(root.find('size').find('height').text)\n",
    "        \n",
    "        # Extract Metadata (Weather/Light) if available\n",
    "        weather = \"Unknown\"\n",
    "        light = \"Unknown\"\n",
    "        meta = root.find('metadata')\n",
    "        if meta is not None:\n",
    "            weather = meta.find('weather').text if meta.find('weather') is not None else \"Unknown\"\n",
    "            light = meta.find('light').text if meta.find('light') is not None else \"Unknown\"\n",
    "\n",
    "        # Extract Objects\n",
    "        for member in root.findall('object'):\n",
    "            class_name = member.find('name').text\n",
    "            bndbox = member.find('bndbox')\n",
    "            \n",
    "            # FIXED: Parse as float first to handle decimal coordinates\n",
    "            xmin = int(float(bndbox.find('xmin').text))\n",
    "            ymin = int(float(bndbox.find('ymin').text))\n",
    "            xmax = int(float(bndbox.find('xmax').text))\n",
    "            ymax = int(float(bndbox.find('ymax').text))\n",
    "            \n",
    "            # Calculate derived stats\n",
    "            bbox_width = xmax - xmin\n",
    "            bbox_height = ymax - ymin\n",
    "            area = bbox_width * bbox_height\n",
    "            aspect_ratio = bbox_width / bbox_height if bbox_height > 0 else 0\n",
    "            \n",
    "            # Normalized center coordinates (0-1) for heatmap\n",
    "            center_x = (xmin + bbox_width/2) / img_width\n",
    "            center_y = (ymin + bbox_height/2) / img_height\n",
    "\n",
    "            xml_list.append({\n",
    "                'filename': filename,\n",
    "                'width': img_width,\n",
    "                'height': img_height,\n",
    "                'class': class_name,\n",
    "                'xmin': xmin,\n",
    "                'ymin': ymin,\n",
    "                'xmax': xmax,\n",
    "                'ymax': ymax,\n",
    "                'bbox_width': bbox_width,\n",
    "                'bbox_height': bbox_height,\n",
    "                'bbox_area': area,\n",
    "                'aspect_ratio': aspect_ratio,\n",
    "                'center_x': center_x,\n",
    "                'center_y': center_y,\n",
    "                'weather': weather,\n",
    "                'light': light\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(xml_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f94cc-03dd-4973-93d2-d44d90c846e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(df):\n",
    "    \"\"\"Plots the count of objects per class.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x='class', palette='viridis', order=df['class'].value_counts().index)\n",
    "    plt.title('Object Class Distribution')\n",
    "    plt.xlabel('Class Name')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a8c7d-193c-4575-8803-25f02cbeee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metadata_distribution(df):\n",
    "    \"\"\"Plots the distribution of Weather and Light conditions.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Weather\n",
    "    sns.countplot(data=df, x='weather', ax=axes[0], palette='coolwarm', order=df['weather'].value_counts().index)\n",
    "    axes[0].set_title('Weather Condition Distribution')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Light\n",
    "    sns.countplot(data=df, x='light', ax=axes[1], palette='magma', order=df['light'].value_counts().index)\n",
    "    axes[1].set_title('Lighting Condition Distribution')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb134e-8888-4517-bf9b-d1bc104419ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bbox_statistics(df):\n",
    "    \"\"\"Plots Box sizes and Aspect Ratios.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 1. BBox Area Distribution\n",
    "    sns.histplot(df['bbox_area'], bins=30, kde=True, ax=axes[0], color='skyblue')\n",
    "    axes[0].set_title('Bounding Box Area Distribution')\n",
    "    axes[0].set_xlabel('Area (pixels)')\n",
    "    \n",
    "    # 2. Aspect Ratio Distribution\n",
    "    sns.histplot(df['aspect_ratio'], bins=30, kde=True, ax=axes[1], color='orange')\n",
    "    axes[1].set_title('Aspect Ratio (Width/Height)')\n",
    "    axes[1].set_xlabel('Ratio')\n",
    "    axes[1].axvline(1.0, color='red', linestyle='--', label='Square')\n",
    "    axes[1].legend()\n",
    "\n",
    "    # 3. Width vs Height Scatter\n",
    "    sns.scatterplot(data=df, x='bbox_width', y='bbox_height', hue='class', alpha=0.6, ax=axes[2])\n",
    "    axes[2].set_title('BBox Width vs Height')\n",
    "    axes[2].plot([0, max(df.bbox_width)], [0, max(df.bbox_width)], 'r--', alpha=0.5) # Diagonal reference\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc48c0f7-43fe-4a9a-b5e4-8423fd353f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_object_heatmap(df):\n",
    "    \"\"\"Plots a 2D histogram of where objects appear in images.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist2d(df['center_x'], df['center_y'], bins=50, cmap='inferno', range=[[0, 1], [0, 1]])\n",
    "    plt.colorbar(label='Count')\n",
    "    plt.gca().invert_yaxis() # Image coordinates: (0,0) is top-left\n",
    "    plt.title('Object Location Heatmap (Normalized Coordinates)')\n",
    "    plt.xlabel('X Position (Normalized 0-1)')\n",
    "    plt.ylabel('Y Position (Normalized 0-1)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c0664-e77c-449e-b1d4-173f66a2d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(df, img_dir, num_samples=3):\n",
    "    \"\"\"Draws bounding boxes on random sample images.\"\"\"\n",
    "    unique_files = df['filename'].unique()\n",
    "    samples = random.sample(list(unique_files), min(len(unique_files), num_samples))\n",
    "    \n",
    "    print(f\"\\nVisualizing {len(samples)} random samples...\")\n",
    "    \n",
    "    for filename in samples:\n",
    "        img_path = img_dir / filename\n",
    "        if not img_path.exists():\n",
    "            print(f\"Warning: Image {filename} not found.\")\n",
    "            continue\n",
    "            \n",
    "        # Get all boxes for this image\n",
    "        img_data = df[df['filename'] == filename]\n",
    "        \n",
    "        # Open Image\n",
    "        im = Image.open(img_path)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        ax.imshow(im)\n",
    "        \n",
    "        # Draw Boxes\n",
    "        for _, row in img_data.iterrows():\n",
    "            width = row['xmax'] - row['xmin']\n",
    "            height = row['ymax'] - row['ymin']\n",
    "            \n",
    "            # Create a Rectangle patch\n",
    "            rect = patches.Rectangle(\n",
    "                (row['xmin'], row['ymin']), \n",
    "                width, height, \n",
    "                linewidth=2, \n",
    "                edgecolor='lime', \n",
    "                facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add Label\n",
    "            label = f\"{row['class']}\"\n",
    "            plt.text(\n",
    "                row['xmin'], \n",
    "                row['ymin'] - 5, \n",
    "                label, \n",
    "                color='white', \n",
    "                fontsize=10, \n",
    "                weight='bold', \n",
    "                bbox=dict(facecolor='lime', alpha=0.5, pad=2, edgecolor='none')\n",
    "            )\n",
    "            \n",
    "        plt.title(f\"Sample: {filename}\\nWeather: {img_data.iloc[0]['weather']} | Light: {img_data.iloc[0]['light']}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb7c22-757c-4349-933f-2a57e5a62c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Annotation directory not found at ../dataset/GERALD_subset/annotations\n",
      "Please run the subset creator script first.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if not ANN_DIR.exists():\n",
    "        print(f\"Error: Annotation directory not found at {ANN_DIR}\")\n",
    "        print(\"Please run the subset creator script first.\")\n",
    "    else:\n",
    "        # 1. Load Data\n",
    "        df = parse_annotations(ANN_DIR)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No annotations found!\")\n",
    "        else:\n",
    "            print(f\"Loaded {len(df)} objects from {df['filename'].nunique()} images.\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            # 2. Basic Stats\n",
    "            print(\"Class Counts:\\n\", df['class'].value_counts())\n",
    "            print(\"\\nWeather Counts:\\n\", df['weather'].value_counts())\n",
    "            \n",
    "            # 3. Visualizations\n",
    "            print(\"\\nGenerating Class Distribution Plot...\")\n",
    "            plot_class_distribution(df)\n",
    "            \n",
    "            print(\"Generating Metadata Distribution Plot...\")\n",
    "            plot_metadata_distribution(df)\n",
    "            \n",
    "            print(\"Generating Bounding Box Statistics...\")\n",
    "            plot_bbox_statistics(df)\n",
    "            \n",
    "            print(\"Generating Object Location Heatmap...\")\n",
    "            plot_object_heatmap(df)\n",
    "            \n",
    "            # 4. Visual Samples\n",
    "            visualize_samples(df, IMG_DIR, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce5c441-9588-4e3c-8f93-e169045a5c0a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fde15-7e75-41ca-8c1b-d8dc8bccf3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d162dd-cbe0-44e1-9a40-779af9384372",
   "metadata": {},
   "source": [
    "## Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612b3fd-d8c3-4033-9ebb-822dfa848683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeraldDataGenerator(keras_utils.Sequence):\n",
    "    \"\"\"\n",
    "    Generates data for Keras on-the-fly to save memory.\n",
    "    Loads, crops, and processes images in batches.\n",
    "    \"\"\"\n",
    "    def __init__(self, samples, batch_size, img_size, lb, num_classes, shuffle=True):\n",
    "        self.samples = samples  # List of (img_path, label, bbox)\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.lb = lb\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.samples))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return math.ceil(len(self.samples) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_samples = [self.samples[k] for k in batch_indexes]\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for img_path, label, bbox in batch_samples:\n",
    "            try:\n",
    "                # Read Image\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None: continue\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                h, w, _ = img.shape\n",
    "                \n",
    "                # Crop using pre-calculated bbox\n",
    "                xmin, ymin, xmax, ymax = bbox\n",
    "                \n",
    "                # Safety clip\n",
    "                xmin, ymin = max(0, xmin), max(0, ymin)\n",
    "                xmax, ymax = min(w, xmax), min(h, ymax)\n",
    "                \n",
    "                if xmax - xmin <= 0 or ymax - ymin <= 0:\n",
    "                    continue\n",
    "\n",
    "                crop = img[ymin:ymax, xmin:xmax]\n",
    "                crop = cv2.resize(crop, self.img_size)\n",
    "                \n",
    "                # Normalize\n",
    "                crop = crop.astype('float32') / 255.0\n",
    "                \n",
    "                X.append(crop)\n",
    "                y.append(label)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not X: # Handle empty batch edge case\n",
    "            return np.zeros((0, *self.img_size, 3)), np.zeros((0, self.num_classes))\n",
    "\n",
    "        X = np.array(X)\n",
    "        # Transform labels using the fitted binarizer\n",
    "        y_encoded = self.lb.transform(y)\n",
    "        \n",
    "        # Ensure correct shape for binary classification if needed\n",
    "        if self.num_classes == 2 and y_encoded.shape[1] == 1:\n",
    "             y_encoded = to_categorical(y_encoded, num_classes=2)\n",
    "        elif self.num_classes == 2:\n",
    "             # If lb returns 2 columns already (rare for 2 classes), handle it\n",
    "             pass\n",
    "\n",
    "        return X, np.array(y_encoded)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Updates indexes after each epoch\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa58653-7410-43be-a89a-7c9245b36a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_dataset_metadata(ann_dir, img_dir):\n",
    "    \"\"\"\n",
    "    Scans XMLs and returns a list of metadata tuple: (img_path, label, bbox).\n",
    "    Does NOT load images into memory.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    labels_list = []\n",
    "    \n",
    "    xml_files = list(ann_dir.glob(\"*.xml\"))\n",
    "    print(f\"Scanning {len(xml_files)} annotation files...\")\n",
    "    \n",
    "    for xml_file in tqdm(xml_files):\n",
    "        try:\n",
    "            tree = ET.parse(xml_file)\n",
    "            root = tree.getroot()\n",
    "            filename = root.find('filename').text\n",
    "            img_path = img_dir / filename\n",
    "            \n",
    "            if not img_path.exists():\n",
    "                continue\n",
    "                \n",
    "            for obj in root.findall('object'):\n",
    "                label = obj.find('name').text\n",
    "                bndbox = obj.find('bndbox')\n",
    "                \n",
    "                xmin = int(float(bndbox.find('xmin').text))\n",
    "                ymin = int(float(bndbox.find('ymin').text))\n",
    "                xmax = int(float(bndbox.find('xmax').text))\n",
    "                ymax = int(float(bndbox.find('ymax').text))\n",
    "                \n",
    "                samples.append((img_path, label, (xmin, ymin, xmax, ymax)))\n",
    "                labels_list.append(label)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {xml_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return samples, labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b867952d-1105-4780-8efb-1264d91c85df",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41854b-ab7d-4508-9e32-5861915b9d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_name, num_classes):\n",
    "    input_tensor = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "    \n",
    "    if model_name == \"ResNet50\":\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "    elif model_name == \"VGG16\":\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "    elif model_name == \"MobileNetV2\":\n",
    "        base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "    elif model_name == \"EfficientNetB0\":\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    \n",
    "    # Use Built-in TF 2.13+ F1Score metric\n",
    "    f1_metric = tf.keras.metrics.F1Score(average='weighted', name='f1_score')\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', f1_metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd687be-0bc8-4c82-b285-999f278657d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(histories):\n",
    "    \"\"\"Plots Accuracy and F1 Score curves.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    for name, history in histories.items():\n",
    "        axes[0].plot(history.history['val_accuracy'], label=f'{name}')\n",
    "    axes[0].set_title('Validation Accuracy')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Plot F1 Score\n",
    "    for name, history in histories.items():\n",
    "        # Check for standard 'f1_score' or 'val_f1_score' keys\n",
    "        metric_key = 'val_f1_score'\n",
    "        \n",
    "        # Sometimes Keras adds indices if multiple metrics exist, handle robustly\n",
    "        keys = history.history.keys()\n",
    "        if metric_key not in keys:\n",
    "            # Try to find a key containing 'f1'\n",
    "            possible_keys = [k for k in keys if 'f1' in k and 'val' in k]\n",
    "            if possible_keys:\n",
    "                metric_key = possible_keys[0]\n",
    "            else:\n",
    "                 print(f\"Warning: F1 metric key not found for {name}. Available: {keys}\")\n",
    "                 continue\n",
    "\n",
    "        axes[1].plot(history.history[metric_key], label=f'{name}')\n",
    "\n",
    "    axes[1].set_title('Validation F1 Score')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('F1 Score')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81fbece-8273-4697-b861-3afe1090c639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning 0 annotation files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found! Please run the subset creator first.\n",
      "\n",
      "Total Samples Found: 0\n",
      "Class distribution: (array([], dtype=float64), array([], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y has 0 samples: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1933943251.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 2. Prepare Labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_raw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Fit once on all data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Classes: {lb.classes_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    307\u001b[0m             )\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y has 0 samples: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y has 0 samples: []"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Scan Metadata (No Image Loading yet)\n",
    "    samples, labels_raw = scan_dataset_metadata(ANN_DIR, IMG_DIR)\n",
    "    \n",
    "    if len(samples) == 0:\n",
    "        print(\"No data found! Please run the subset creator first.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"\\nTotal Samples Found: {len(samples)}\")\n",
    "    print(f\"Class distribution: {np.unique(labels_raw, return_counts=True)}\")\n",
    "    \n",
    "    # 2. Prepare Labels\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(labels_raw) # Fit once on all data\n",
    "    num_classes = len(lb.classes_)\n",
    "    print(f\"Classes: {lb.classes_}\")\n",
    "    \n",
    "    # 3. Split Metadata\n",
    "    train_samples, test_samples = train_test_split(\n",
    "        samples, test_size=0.2, random_state=42, stratify=labels_raw\n",
    "    )\n",
    "    \n",
    "    # 4. Create Generators\n",
    "    train_gen = GeraldDataGenerator(train_samples, BATCH_SIZE, IMG_SIZE, lb, num_classes, shuffle=True)\n",
    "    test_gen = GeraldDataGenerator(test_samples, BATCH_SIZE, IMG_SIZE, lb, num_classes, shuffle=False)\n",
    "    \n",
    "    # 5. Train Models\n",
    "    models_to_train = [\"ResNet50\", \"VGG16\", \"MobileNetV2\", \"EfficientNetB0\"]\n",
    "    histories = {}\n",
    "    results = []\n",
    "    \n",
    "    for model_name in models_to_train:\n",
    "        print(f\"\\n{'='*20} Training {model_name} {'='*20}\")\n",
    "        \n",
    "        model = build_model(model_name, num_classes)\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=test_gen,\n",
    "            verbose=1\n",
    "        )\n",
    "        histories[model_name] = history\n",
    "        \n",
    "        # Evaluate using generator (Need prediction loop for sklearn metrics)\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        \n",
    "        # Re-instantiate test gen with shuffle=False to ensure order matches for evaluation\n",
    "        eval_gen = GeraldDataGenerator(test_samples, BATCH_SIZE, IMG_SIZE, lb, num_classes, shuffle=False)\n",
    "        \n",
    "        y_pred_prob = model.predict(eval_gen)\n",
    "        y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "        \n",
    "        # Extract ground truth from test_samples manually for comparison\n",
    "        y_test_labels = [s[1] for s in test_samples]\n",
    "        y_test_encoded = lb.transform(y_test_labels)\n",
    "        if num_classes == 2:\n",
    "             # Handle binary shape differences from sklearn vs keras\n",
    "             if y_test_encoded.shape[1] == 1:\n",
    "                 # If binary, argmax won't work on (N,1). We need 0 or 1 directly.\n",
    "                 # But model.predict outputs (N, 2) usually if we force to_categorical in generator.\n",
    "                 # Let's align:\n",
    "                 y_true = y_test_encoded.ravel()\n",
    "             else:\n",
    "                 y_true = np.argmax(y_test_encoded, axis=1)\n",
    "        else:\n",
    "             y_true = np.argmax(y_test_encoded, axis=1)\n",
    "             \n",
    "        # Robustness check for sizes\n",
    "        min_len = min(len(y_true), len(y_pred))\n",
    "        y_true = y_true[:min_len]\n",
    "        y_pred = y_pred[:min_len]\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": acc,\n",
    "            \"F1-Score (Weighted)\": f1\n",
    "        })\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=lb.classes_, yticklabels=lb.classes_)\n",
    "        plt.title(f'Confusion Matrix: {model_name}')\n",
    "        plt.show()\n",
    "\n",
    "    # 6. Final Comparison\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*40)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df)\n",
    "    \n",
    "    plot_history(histories)\n",
    "    \n",
    "    best_model_name = results_df.sort_values(by=\"Accuracy\", ascending=False).iloc[0][\"Model\"]\n",
    "    print(f\"\\nBest performing model: {best_model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
